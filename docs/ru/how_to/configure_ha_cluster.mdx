---
weight: 20
sourceSHA: 01937b88b408164f0a8bb985310c98fad9289f18d8c068253ca3c0be6647b697
---

# Настройка кластера PostgreSQL с высокой доступностью

## Обзор

Данное руководство объясняет, как настроить кластер PostgreSQL с высокой доступностью в Kubernetes, используя Patroni для автоматического переключения при сбоях и управления кластером, с поддержкой синхронной репликации, пула соединений, мониторинга и многого другого.

## Предварительные требования

1. Должен быть установлен оператор PostgreSQL
2. Должен быть настроен класс хранения, поддерживающий динамическоеProvisioning
3. Необходимы разрешения на создание ресурсов CRD
4. Убедитесь, что кластер Kubernetes имеет достаточные ресурсы (как минимум 3 доступных узла)
5. Должно быть настроено хранилище для резервного копирования (S3 или совместимое хранилище)

## Процедура

<Tabs>
  <Tab label="CLI">
    ### 1. Создание кластера с высокой доступностью

    ```bash
    cat <<EOF | kubectl create -n $NAMESPACE -f -
    apiVersion: acid.zalan.do/v1
    kind: postgresql
    metadata:
      name: pg-ha-cluster
    spec:
      teamId: ACID
      enableExporter: true
      enablePgpool2: false
      spiloPrivileged: false
      spiloRunAsGroup: 103
      spiloRunAsUser: 101
      spiloAllowPrivilegeEscalation: false
      enableReadinessProbe: true
      numberOfInstances: 3
      postgresql:
        version: "14"
        parameters:
          shared_buffers: "1GB"
          work_mem: "64MB"
      resources:
        requests:
          cpu: "1"
          memory: 2Gi
        limits:
          cpu: "2"
          memory: 4Gi
      volume:
        size: 50Gi
        storageClass: ssd
        iops: 3000
        throughput: 125
      patroni:
        ttl: 30
        loop_wait: 10
        retry_timeout: 10
        maximum_lag_on_failover: 1048576
        synchronous_mode: true
        synchronous_node_count: 2
        pg_hba:
          - host all all 0.0.0.0/0 md5
      backup:
        schedule: "0 0 * * *"
        retainDay: 7
        storage:
          bucket: "my-backups"
          name: "backup-storage"
          namespace: "default"
    EOF
    ```

    ### 2. Проверка статуса кластера

    ```bash
    kubectl -n $NAMESPACE get postgresql pg-ha-cluster -o yaml
    ```

    Пример вывода:

    ```yaml
    status:
      PostgresClusterStatus: Running
      pods:
      - pg-ha-cluster-0
      - pg-ha-cluster-1
      - pg-ha-cluster-2
      master: pg-ha-cluster-0
      patroniStatus:
        pg-ha-cluster-0:
          role: leader
          state: running
          replication:
            state: streaming
            sync_state: sync
        pg-ha-cluster-1:
          role: replica
          state: running
          replication:
            state: streaming
            sync_state: async
    ```

    ### 3. Настройка пула соединений (необязательно)

    ```bash
    kubectl patch postgresql pg-ha-cluster --type='merge' -p '
    spec:
      enableConnectionPooler: true
      connectionPooler:
        numberOfInstances: 2
        mode: "transaction"
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "1"
            memory: "1Gi"
    '
    ```

    ### 4. Настройка мониторинга (необязательно)

    ```bash
    kubectl patch postgresql pg-ha-cluster --type='merge' -p '
    spec:
      enableExporter: true
      exporter:
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
    '
    ```
  </Tab>

  <Tab label="Веб-консоль">
    1. В левой боковой панели нажмите **PostgreSQL**
    2. Выберите целевое пространство имен
    3. Нажмите **Создать экземпляр PostgreSQL**
    4. Настройте следующие параметры:

    <table border="">
      <tr>
        <th>Элемент конфигурации</th>
        <th>Описание</th>
      </tr>

      <tr>
        <td>Имя экземпляра</td>
        <td>Уникальный идентификатор для кластера</td>
      </tr>

      <tr>
        <td>Версия PostgreSQL</td>
        <td>Выберите поддерживаемую версию PostgreSQL</td>
      </tr>

      <tr>
        <td>Количество реплик</td>
        <td>Установите количество узлов кластера (рекомендуется 3 или более)</td>
      </tr>

      <tr>
        <td>Спецификации (ограничения контейнера)</td>
        <td>Установите ограничения по ресурсам CPU и памяти</td>
      </tr>

      <tr>
        <td>Класс хранения</td>
        <td>Выберите класс хранения, поддерживающий высокие IOPS</td>
      </tr>

      <tr>
        <td>Квота на хранение</td>
        <td>Установите размер хранилища (рекомендуется 50 ГБ или более)</td>
      </tr>
    </table>

    5. Нажмите **Создать** и ожидайте, пока статус экземпляра не станет **Рабочим**
    6. Просмотрите статус кластера и информацию об узлах на странице деталей экземпляра
  </Tab>
</Tabs>

## Ключевые параметры

### Конфигурация кластера

| Параметр               | Значение по умолчанию | Описание                                       |
| --------------------- | ------------------- | ----------------------------------------------- |
| numberOfInstances     | 3                   | Количество узлов кластера (рекомендуется нечётное число) |
| postgresql.version    | "14"                | Основная версия PostgreSQL                    |
| postgresql.parameters | {}                  | Параметры конфигурации PostgreSQL             |
| resources             | -                   | Запросы и ограничения ресурсов                |
| volume.size           | -                   | Размер тома хранилища                         |
| volume.storageClass   | -                   | Название класса хранения                       |
| volume.iops           | -                   | IOPS (только для классов хранения, поддерживающих IOPS) |

### Конфигурация Patroni

| Параметр                          | Значение по умолчанию | Описание                                     |
| ---------------------------------- | ------------------- | --------------------------------------------- |
| patroni.ttl                        | 30                  | Время аренды главного узла (в секундах)      |
| patroni.loop\_wait                 | 10                  | Интервал проверки состояния (в секундах)      |
| patroni.retry\_timeout             | 10                  | Таймаут повторной попытки (в секундах)       |
| patroni.maximum\_lag\_on\_failover | 1048576             | Максимально допустимая задержка репликации (в байтах) |
| patroni.synchronous\_mode          | false               | Возможность включения синхронной репликации  |
| patroni.synchronous\_node\_count   | 1                   | Количество узлов синхронной репликации       |
| patroni.pg\_hba                    | \[]                 | Пользовательская конфигурация pg\_hba        |

### Конфигурация резервного копирования

| Параметр                | Значение по умолчанию | Описание                           |
| ------------------------ | ------------------- | --------------------------------- |
| backup.schedule          | -                   | Расписание резервного копирования (формат cron)   |
| backup.retainDay         | 7                   | Дни хранения резервных копий     |
| backup.storage.bucket    | -                   | Название резервного хранилища   |
| backup.storage.name      | -                   | Название конфигурации хранилища |
| backup.storage.namespace | -                   | Пространство имен конфигурации хранилища |

## Проверка результата

1. Проверьте, что статус кластера - Рабочий
2. Подтвердите, что все Pods работают нормально
3. Проверьте статус Patroni:
   ```bash
   kubectl exec pg-ha-cluster-0 -- patronictl list
   ```
4. Протестируйте переключение при сбоях:
   ```bash
   kubectl delete pod pg-ha-cluster-0
   ```
   Наблюдайте за процессом выборов нового лидера
5. Проверьте статус синхронной репликации:
   ```bash
   kubectl exec pg-ha-cluster-0 -- psql -c "SELECT * FROM pg_stat_replication;"
   ```

## Рекомендации по лучшим практикам

1. Для производственных сред используйте SSD-хранилище с соответствующей конфигурацией IOPS
2. Настройте ограничения по ресурсам с буфером 20-30%
3. Регулярно тестируйте процедуры переключения при сбоях и восстановления
4. Настройте системы мониторинга для предупреждений о:
   - Задержке между основным узлом и репликами
   - Количестве соединений
   - Использовании диска
   - Использовании CPU/памяти
5. Включите запланированные резервные копирования и протестируйте процедуры восстановления
6. При использовании синхронной репликации настройте как минимум 2 синхронные реплики
7. Регулярно проводите оптимизацию производительности и настройку параметров

## Узнайте больше

- [Конфигурация мониторинга](../functions/35_monitor.mdx)
- [Резервное копирование и восстановление](../functions/15_back_restore.mdx)
